{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5b2e3f",
   "metadata": {},
   "source": [
    "Notebook created by [Nikolaos Tsopanidis](https://github.com/tSopermon)\n",
    "\n",
    "# RAG System: Data Ingestion\n",
    "\n",
    "Data ingestion is the first step in building a RAG system that will provide reliable data source to an LLM and enhance the capabilities of a pre-trained LLM.\n",
    "\n",
    "For this task we need to import text-based documents referring to concert tours of 2025-2026 and store the information. The user will be able to ask questions to the LLM and receive information based on the reliable knowledge source which was created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912ed01",
   "metadata": {},
   "source": [
    "## Data ingestion process\n",
    "In this process we will be able to load external documents and store them in a vector database for retrieval. The steps followed are:\n",
    " 1. **Load**: Loading data into documents  \n",
    " 2. **Split**: Splitting data into manageable chunks  \n",
    " 3. **Embed**: Creating document embeddings  \n",
    " 4. **Store**: Storing embeddings into a vector database \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "![Alt](https://miro.medium.com/v2/resize:fit:720/format:webp/1*wHqtILSjqYsF6RnDq2CJDA.png)\n",
    "\n",
    "Source: [medium.com - Amina Javaid](https://medium.com/@aminajavaid30/building-a-rag-system-the-data-ingestion-pipeline-d04235fd17ea)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ef5b5",
   "metadata": {},
   "source": [
    "## LangChain Implementation\n",
    "**LangChain** is an open source framework used commonly in GenAI applications. Not only we can build apps, but we can use LangSmith, introduced by LangChain, to monitor LLMs, debug and evaluate code.\n",
    "\n",
    "* First, we will install the required packages for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5768e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies if not already installed\n",
    "%pip install langchain-community langchain-experimental langchain-huggingface faiss-cpu tiktoken sentence_transformers chromadb huggingface_hub[hf_xet] langchain-huggingface langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c7069",
   "metadata": {},
   "source": [
    "### **1. Data Loading**\n",
    "Data extraction and convertion into a suitable format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363202aa",
   "metadata": {},
   "source": [
    "#### **Loaders**\n",
    "LangChain's loaders facilitate data ingestion and preprocessing from various sources. Data is loaded into a document object which includes the text content and the associated metadata. Loaders can also perform tokenization, normalization and format convertion, preparing data for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95eddcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s) from data/my_document.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loading text documents using LangChain Community.\n",
    "\"\"\"\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "file_path = \"data/my_document.txt\" # example file path\n",
    "loader = TextLoader(file_path) # Load the document\n",
    "documents = loader.load() # Load the documents\n",
    "print(f\"Loaded {len(documents)} document(s) from {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7263d",
   "metadata": {},
   "source": [
    "##### **Metadata** can prove useful when building an app where users need to retrieve information along with source or page location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c7884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: data/my_document.txt\n",
      "Document Content: COVENANT\n",
      "\n",
      " \n",
      "\n",
      "06.09.2025 Doors: 7:30 pm, Petra's Theater, M. Merkouri, Petroupoli 132 31, Athens\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "Document Metadata: {'source': 'data/my_document.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata created by the loader\n",
    "for doc in documents:\n",
    "    print(f\"Document ID: {doc.metadata['source']}\")\n",
    "    print(f\"Document Content: {doc.page_content[:100]}...\")  # Print first 100 characters\n",
    "    print(f\"Document Metadata: {doc.metadata}\")  # Print all metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8a5ba",
   "metadata": {},
   "source": [
    "### **2. Data Splitting**\n",
    "When preprocessing data for a RAG system, its important to consider the data's characteristics, system requirements, and limitations. Large documents may won't be processed by an LLM due to context window limits. To handle this, data must be split into smaller chunks while preserving context for effective retrieval. The reasons are:\n",
    " 1. **Manageability**: Smaller parts will fit better within the LLM's context window, ensuring easier handling\n",
    " 2. **Embedding model compatability**: Embedded models are limited on the amount of data they can process at a time, chunking the data offers better alignmnet with the model's capacity.\n",
    " 3. **Efficient retrieval** When user sends a query, they only need specific information, often found in smaller chunks. Retrieving smaller chunks will decrease the resources needed instead of processing an entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0369fe82",
   "metadata": {},
   "source": [
    "#### **Splitters**\n",
    "Splitters divide large text documents into smaller chunks, ensuring they fit within model constraints and can be processed individually. Chunk size must be chosen carefully to preserve context. Overlapping chunks help maintain continuity, while minimal overlap works for distinct topics, ensuring high-quality responses.\n",
    "\n",
    "We can adjust the chunk size depending on the limitations of the embedding model in use. Embedding models can be found on platforms like [Hugging Face](https://huggingface.co/models?other=embeddings), where each has specific input and output token limits\n",
    "* Types of splitters: \n",
    "    - Recursive Splitter, \n",
    "    - HTML Splitter, \n",
    "    - Markdown Splitter, \n",
    "    - Code Splitter, \n",
    "    - Token Splitter, \n",
    "    - Character Splitter, \n",
    "    - Semantic Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604c8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\"\"\"\n",
    "CharacterTextSplitter splits text into chunks based on character count.\n",
    "https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html\n",
    "\"\"\"\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  # Split on double newlines\n",
    "    chunk_size=400,  # Maximum size of each chunk\n",
    "    chunk_overlap=50,  # Overlap between chunks\n",
    "    length_function=len,  # Function to calculate the length of the text\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    texts = text_splitter.split_text(doc.page_content)\n",
    "    chunks.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a191c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: COVENANT\n",
      "\n",
      " \n",
      "\n",
      "06.09.2025 Doors: 7:30 pm, Petra's Theater, M. Merkouri, Petroupoli 132 31, Athens\n",
      "\n",
      "INFO\n",
      " \n",
      "\n",
      "The legendary Covenant at Petra's Theatre – Saturday, September 6, 2025\n",
      "\n",
      " \n",
      "“Covenant, one of the leading names in EBM and dark electronic music, is coming to Athens for a unique performance at the Petra's Theatre on Saturday, September 6, 2025.\n",
      "Chunk 1 Length: 349\n",
      "--------------------------------------------------\n",
      "Chunk 2: With a career spanning over three decades, Covenant has established itself as one of the most influential bands in electronic music. From the iconic \"Like Tears in Rain,\" \"Sequencer,\" \"Northern Light,\" and \"United States of Mind\" to their more recent releases, the Swedish band continues to redefine their sound, always maintaining their signature intensity and atmosphere.\n",
      "Chunk 2 Length: 373\n",
      "--------------------------------------------------\n",
      "Chunk 3: With tracks like \"Dead Stars,\" \"Call the Ships to Port,\" \"Bullet,\" \"Theremin,\" and many more, Covenant promises an evening full of energy, dark rhythms, and pure dancefloor excitement, just as they always deliver.\n",
      "\n",
      " \n",
      "\n",
      "Be part of a magical evening with Covenant, and enjoy a live experience that will captivate you!\n",
      "Organized by TRUE ART.\n",
      "Chunk 3 Length: 337\n",
      "--------------------------------------------------\n",
      "Chunk 4: EXTRA INFO\n",
      " \n",
      "Access by Public Transport:\n",
      " \n",
      "Bus: 700, 747, A13, B11, A11\n",
      "Metro: M2\n",
      "Trolley: 24\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "ΤICKET PRICES\n",
      " \n",
      "€15 | Early Bird (Limited number)\n",
      "€20 | Presale\n",
      "€25 | On Spot\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "TICKET PRESALE\n",
      "Chunk 4 Length: 200\n",
      "--------------------------------------------------\n",
      "Chunk 5: TICKET PRESALE\n",
      " \n",
      "\n",
      "Opening Act TBAThe official ticket sales points for this event are as follows. Please note that Ticketmaster assumes no responsibility for tickets obtained from sources outside its official network and cannot guarantee their accuracy or authenticity.\n",
      "Chunk 5 Length: 268\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print the first 5 chunks\n",
    "for i, chunk in enumerate(chunks[:5]):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "    print(f\"Chunk {i+1} Length: {len(chunk)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56302365",
   "metadata": {},
   "source": [
    "### **3. Embeddings**\n",
    "Textual data is converted into vector embeddings for efficient retrieval in a vector database. These dense, multi-dimensional vectors capture the semantic meaning of words, phrases, or sentences. LLMs generate embeddings, where semantically similar words have closer vector representations. Embeddings, often created using transformer-based models, can also detect anomalies by identifying outliers. Models are available as open-source on **Hugging Face** or paid options like **OpenAI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2cc890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of the HuggingFaceEmbeddings model\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\"\"\"\n",
    "Source: https://huggingface.co/BAAI/bge-m3\n",
    "\n",
    "BG3-M3 offers:\n",
    "* Multi-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense \n",
    "  retrieval, multi-vector retrieval, and sparse retrieval.\n",
    "* Multi-Linguality: It can support more than 100 working languages.\n",
    "* Multi-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long \n",
    "  documents of up to 8192 tokens.\n",
    "\n",
    "Suggestions for retrieval pipeline in RAG (future work):\n",
    "* Hybrid retrieval leverages the strengths of various methods, offering higher accuracy and stronger generalization \n",
    "  capabilities. A classic example: using both embedding retrieval and the BM25 algorithm. Now, you can try to \n",
    "  use BGE-M3, which supports both embedding and sparse retrieval. This allows you to obtain token weights (similar to \n",
    "  the BM25) without any additional cost when generate dense embeddings. To use hybrid retrieval, you can refer to \n",
    "  Vespa (https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/mother-of-all-embedding-models-cloud.ipynb) \n",
    "  and Milvus.\n",
    "* As cross-encoder models, re-ranker demonstrates higher accuracy than bi-encoder embedding model. Utilizing the \n",
    "  re-ranking model (e.g., bge-reranker, bge-reranker-v2) after retrieval can further filter the selected text.\n",
    "\"\"\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs={\n",
    "        \"device\": \"cuda:0\"  # Use GPU if available\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create embeddings of text chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Text chunk {i+1}\")\n",
    "    print(50*\"-\")\n",
    "    print(chunk, \"\\n\")\n",
    "    query_result = embeddings.embed_query(chunk)\n",
    "    print(f\"Embeddings: {query_result}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e2050",
   "metadata": {},
   "source": [
    "We will choose work with all-mpnet-base-v2 model on this project because BG3-M3 model'size is 2.27 GB, while all-mpnet-base-v2 model's size is 438 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fdc203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs = {\n",
    "        \"device\": \"cuda:0\"  # Use GPU if available\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(\"Text chunk \", i)\n",
    "    print(\"--------------\")\n",
    "    print(chunk, \"\\n\")\n",
    "    query_result = embeddings.embed_query(chunk)\n",
    "    print(\"Embeddings\", \"\\n\", query_result, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3035824",
   "metadata": {},
   "source": [
    "### **4. Data Storage - Vector Stores**\n",
    "Vector stores efficiently store and query high-dimensional vector data, making them essential for AI applications like recommendation systems and RAG. Unlike traditional databases, which organize data in rows and columns, vector stores handle unstructured data like text, images, or speech more effectively. They are widely used in machine learning and retrieval tasks to find semantically similar information.\n",
    "\n",
    "Vector store structure:\n",
    " * Vectors/Embeddings\n",
    " * Metadata\n",
    " * Original data\n",
    " * Unique ID\n",
    "\n",
    "Vector stores use two main operations for querying:\n",
    "- **Cosine Similarity**: Identifies top-k similar vectors by comparing the input vector (e.g., product description) with stored vectors. Metadata retrieves relevant details, commonly used in recommendation systems.\n",
    "- **Dot Product**: Another method for comparing vectors effectively.\n",
    "\n",
    "For this project, we will create a locally managed vector database suing [ChromaDB](https://github.com/chroma-core/chroma). Cloud-based services come with associated cloud sercice costs. Chroma is free and can run locally or on a remote server and it is ideal for flexible, LLM-focused tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe5c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\"\"\"\n",
    "https://api.python.langchain.com/en/latest/modules/indexes/vectorstores/langchain_community.vectorstores.chroma.Chroma.html\n",
    "\"\"\"\n",
    "vector_store = Chroma.from_texts(\n",
    "    texts=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"data/chroma_db\",  # Directory to persist the database\n",
    "    collection_name=\"my_collection\"  # Name of the collection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2557247",
   "metadata": {},
   "source": [
    "## **References**\n",
    "* https://medium.com/@aminajavaid30/building-a-rag-system-the-data-ingestion-pipeline-d04235fd17ea\n",
    "* https://medium.com/@laddhaakshatrai/how-to-perform-data-ingestion-with-langchain-day-12-100-f11288d7ae99\n",
    "* https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "* https://huggingface.co/mrhimanshu/finetuned-bge-m3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf6c8da",
   "metadata": {},
   "source": [
    "According to this [colab notebook](https://colab.research.google.com/drive/1gyGZn_LZNrYXYXa-pltFExbptIe7DAPe?usp=sharing) by Sam Witteveen, we continue the process below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae46c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75580/4279230521.py:2: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_store.persist()\n"
     ]
    }
   ],
   "source": [
    "# persist the database (save it to disk)\n",
    "# vector_store.persist()\n",
    "# Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04561dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the persisted database from disk to use it\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"data/chroma_db\",  # Directory to persist the database\n",
    "    collection_name=\"my_collection\"  # Name of the collection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de59418",
   "metadata": {},
   "source": [
    "**Let's create a retriever interface using vector store**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cea789f",
   "metadata": {},
   "source": [
    "### **5. Retriever Creation**\n",
    "Component responsible for fetching relevant data or documents based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dde0d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})  # Retrieve top 1 most relevant document for a given query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9e4287",
   "metadata": {},
   "source": [
    "### **6. Creating a Chain**\n",
    "https://medium.com/@jiangan0808/retrieval-augmented-generation-rag-with-open-source-hugging-face-llms-using-langchain-bd618371be9d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99327704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f384e82b",
   "metadata": {},
   "source": [
    "Setting a [HuggingFace Pipeline](https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edbb92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/flan-t5-small\",\n",
    "    task=\"text2text-generation\",\n",
    "    pipeline_kwargs={\"max_length\": 200},\n",
    ")\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67986b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = \"What is the main topic of the document?\"\n",
    "result = qa.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f30695f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Covenant\n",
      "Sources: ['Unknown source']\n"
     ]
    }
   ],
   "source": [
    "# Print the answer and sources\n",
    "answer = result[\"result\"]\n",
    "print(\"Answer:\", answer)\n",
    "if \"source_documents\" in result:\n",
    "    sources = [doc.metadata.get(\"source\", \"Unknown source\") for doc in result[\"source_documents\"]]\n",
    "    print(\"Sources:\", sources)\n",
    "else:\n",
    "    print(\"No source documents returned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e616c",
   "metadata": {},
   "source": [
    "### LangChain [Tutorial](https://python.langchain.com/docs/tutorials/rag/) On how to build a RAG app "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Data)",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
