{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212255cf",
   "metadata": {},
   "source": [
    "Notebook created by [Nikolaos Tsopanidis](https://github.com/tSopermon)\n",
    "\n",
    "# RAG System\n",
    "\n",
    "Data ingestion is the first step in building a RAG system that will provide reliable data source to an LLM and enhance the capabilities of a pre-trained LLM.\n",
    "\n",
    "For this task we need to import text-based documents referring to concert tours of 2025-2026 and store the information. The user will be able to ask questions to the LLM and receive information based on the reliable knowledge source which was created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b5c78",
   "metadata": {},
   "source": [
    "## Data ingestion process\n",
    "In this process we will be able to load external documents and store them in a vector database for retrieval. The steps followed are:\n",
    " 1. **Load**: Loading data into documents  \n",
    " 2. **Split**: Splitting data into manageable chunks  \n",
    " 3. **Embed**: Creating document embeddings  \n",
    " 4. **Store**: Storing embeddings into a vector database \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "![Alt](https://miro.medium.com/v2/resize:fit:720/format:webp/1*wHqtILSjqYsF6RnDq2CJDA.png)\n",
    "\n",
    "Source: [medium.com - Amina Javaid](https://medium.com/@aminajavaid30/building-a-rag-system-the-data-ingestion-pipeline-d04235fd17ea)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348672e2",
   "metadata": {},
   "source": [
    "## LangChain Implementation\n",
    "**LangChain** is an open source framework used commonly in GenAI applications. Not only we can build apps, but we can use LangSmith, introduced by LangChain, to monitor LLMs, debug and evaluate code.\n",
    "\n",
    "* First, we will install the required packages for the project, shown in the `requirements` section below:\n",
    "```bash\n",
    "    streamlit==1.44.1\n",
    "    langchain-ollama==0.3.2\n",
    "    langchain-chroma==0.2.3\n",
    "    transformers==4.51.3\n",
    "    langchain-core==0.3.52\n",
    "    langchain-text-splitters==0.3.8\n",
    "    langchain==0.3.23\n",
    "    torch==2.6.0\n",
    "    serpapi==0.1.5\n",
    "    google-search-results==2.4.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eaac83",
   "metadata": {},
   "source": [
    "## 1. Importing necessary libraries for text processing and vector storage\n",
    "- `re`: Regular expressions for text manipulation.\n",
    "- `Document`: Class for handling documents in the LangChain framework.\n",
    "- `CharacterTextSplitter`: Class for splitting text into smaller chunks based on characters.\n",
    "- `Chroma`: Class for storing and managing vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbfea203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984a11e",
   "metadata": {},
   "source": [
    "## 2. Importing the library to create flexible chat prompts for LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed687bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f33e7e",
   "metadata": {},
   "source": [
    "## 3. Importing libraries to create a retrieval chain. Chains are used to combine multiple components into a single workflow.\n",
    "Chains encode sequencies of calls to LLMs, tools, and other chains.\n",
    "In this case, we will create a retrieval chain that **retrieves relevant documents** from a vector store and then generates a response based on those documents.\n",
    "The **retrieval chain** will use a retriever to find relevant documents and then use a language model to generate a response based on those documents.\n",
    "The **history aware retriever** will be used to keep track of the conversation history and use it to improve the retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5c0eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda733f",
   "metadata": {},
   "source": [
    "## 4. Importing Streamlit to create the web app UI for handling user input and SerpAPI for web search and scraping events from Google Events engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2594f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from serpapi import GoogleSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c4ee3",
   "metadata": {},
   "source": [
    "## 5. Importing LangChain Ollama and Transformers libraries\n",
    "This service uses the Ollama and Transformers libraries to create a chatbot that can answer questions based on a given document.\n",
    "The chatbot uses the Ollama library to generate embeddings for the document and the Transformers library to generate responses based on the embeddings.\n",
    "It also uses the LangChain library to create a retrieval chain that retrieves relevant documents based on the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac77325",
   "metadata": {},
   "source": [
    "## 6. Setting the SerApi API key to use the engine and retrieve events\n",
    "SepApi uses the SerpApi API key to access the Google Search API. You can get your own API key from https://serpapi.com/manage-api-key. Make sure to set the environment variable SERPAPI_API_KEY to your own API key before running this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPAPI_API_KEY = \"e943f5221ef3d91e9eecf2076e110ed0a2615e29ae8919c1d73ad6b24b033534\" # Replace with your own API key\n",
    "os.environ[\"SERPAPI_API_KEY\"] = SERPAPI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b2bee7",
   "metadata": {},
   "source": [
    "## 7. Configuring models for the embeddings and LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f15e1c",
   "metadata": {},
   "source": [
    "### 7.1. Embeddings\n",
    "The embeddings model is used to convert text into vector representations. This is important for semantic search and retrieval tasks.\n",
    "The `OllamaEmbeddings` class is used to create embeddings using the Ollama model. The `model` parameter specifies the name of the model to use. In this case, we are using the `nomic-embed-text` model, which is designed for text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e33f5809",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10af65",
   "metadata": {},
   "source": [
    "### 7.2. LLM\n",
    "The LLM (Language Model) is used to generate text based on the input provided. The `ChatOllama` class is used to create a chat-based language model using the Ollama model. The `model` parameter specifies the name of the model to use. In this case, we are using the `llama3.1:8b` model, which is a variant of the LLaMA model with 8 billion parameters. The `grounding` parameter specifies the grounding method to use. In this case, we are using \"strict\" grounding, which means that the model will be more focused on the input context.\n",
    "The LLM will also be used to generate concise summaries of the uploaded documents and scraped events. Summaries will then be saved in the Chroma vector store to be used for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "688476b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.1:8b\", grounding=\"strict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e7be6",
   "metadata": {},
   "source": [
    "**Important**: `nomic-embed-text` and `llama3.1:8b` need to be downloaded first by installing [Ollama](https://ollama.com/download) in your system and running the following commands:\n",
    "```bash\n",
    "ollama pull nomic-embed-text\n",
    "ollama pull llama3.1:8b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3f32c",
   "metadata": {},
   "source": [
    "## 8. Functions for Document Processing and Vector Base Creation\n",
    "\n",
    "### 8.1. Function to check if the text is concert-related\n",
    "This function checks if the text contains any concert-related keywords. If the text is related to concerts, the app will not load the document, ensuring that only relevant information is processed.\n",
    "This is important for maintaining the focus of the application and ensuring that the user receives accurate and relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c169697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 8. Functions for Document Processing and Vector Base Creation\n",
    "\n",
    "### 8.1. Function to check if the text is concert-related\n",
    "This function checks if the text contains any concert-related keywords. If the text is related to concerts, the app will not load the document, ensuring that only relevant information is processed.\n",
    "This is important for maintaining the focus of the application and ensuring that the user receives accurate and relevant information.\n",
    "\"\"\"\n",
    "def is_concert_related(text, CONCERT_RELATED_KEYWORDS):\n",
    "    text_lower = text.lower() # Convert to lowercase for case-insensitive matching\n",
    "    return any(keyword in text_lower for keyword in CONCERT_RELATED_KEYWORDS)\n",
    "\n",
    "CONCERT_RELATED_KEYWORDS = [\"concert\", \"music\", \"band\", \"performance\", \"stage\", \n",
    "                            \"ticket\", \"venue\", \"artist\", \"festival\", \"tour\", \"gig\", \n",
    "                            \"show\", \"orchestra\", \"symphony\", \"recital\", \"live music\", \n",
    "                            \"audience\", \"encore\", \"setlist\", \"soundcheck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7f694",
   "metadata": {},
   "source": [
    "### 8.2. Function to create a Chroma vector store\n",
    "This function takes a text input and an embeddings model as arguments. It processes the text to remove unnecessary whitespace and non-alphanumeric characters, splits the text into smaller chunks, and creates a Chroma vector store using the provided embeddings model.\n",
    "We will use `RecursiveCharacterTextSplitter` to split the text into smaller chunks as the concert documents consist of multiple paragraphs. This way we will maintain each paragraph semantically intact and avoid splitting them into smaller pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1519a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(text, embeddings):\n",
    "    processed_text = re.sub(r'\\s+', ' ', text)\n",
    "    processed_text = re.sub(r'[^a-zA-Z0-9\\s]', '', processed_text)\n",
    "    documents = [Document(page_content=processed_text)] # list of Document objects\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,\n",
    "                                                   chunk_overlap=20,\n",
    "                                                   length_function=len)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "    vector_store = Chroma.from_texts(texts=chunk_texts,\n",
    "                                     embedding=embeddings,\n",
    "                                     persist_directory=\"chroma_db\",     # directory to persist the database\n",
    "                                     collection_name=\"my_collection\")   # name of the collection\n",
    "    \n",
    "    return vector_store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Data)",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
