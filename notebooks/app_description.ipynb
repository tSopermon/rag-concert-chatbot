{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "212255cf",
   "metadata": {},
   "source": [
    "Notebook created by [Nikolaos Tsopanidis](https://github.com/tSopermon)\n",
    "\n",
    "# Retrieval-Augmented Generation (RAG) App\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a powerful technique used in applications like Q&A chatbots. It combines data retrieval with language model generation to provide accurate and context-aware answers. A typical RAG system consists of two main components:\n",
    "\n",
    "1. **Indexing**: This process prepares the data for efficient retrieval:\n",
    "    * Load: Use Document Loaders to ingest raw data.\n",
    "    * Split: Break large documents into smaller chunks using text splitters, improving searchability and ensuring compatibility with the model's context window.\n",
    "    * Store: Save and index the chunks in a VectorStore using an Embeddings model for later retrieval.\n",
    "\n",
    "2. **Retrieval and Generation**: This process handles user queries at runtime:\n",
    "    * Retrieve: Fetch relevant chunks from the VectorStore using a Retriever based on the user query.\n",
    "    * Generate: Use a ChatModel or LLM to generate an answer by combining the query with the   retrieved data in a prompt.\n",
    "    * This workflow ensures that the system can efficiently process large datasets and provide accurate, contextually relevant responses.\n",
    "\n",
    "    <img src=\"https://cdn.prod.website-files.com/61082de7b224bb1768edad68/6704484409635e6a0dbf4098_65982e165de858b7c41f4fa3_Img%25201.webp\" style=\"height:600px\">\n",
    "    \n",
    "    [image source](https://medium.com/@yogeshkd/four-ways-to-embed-images-in-your-jupyter-notebook-powered-blog-2d28f6d1b6e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61caf598",
   "metadata": {},
   "source": [
    "## LangChain Implementation\n",
    "**LangChain** is an open source framework used commonly in GenAI applications. Not only we can build apps, but we can use LangSmith, introduced by LangChain, to monitor LLMs, debug and evaluate code.\n",
    "\n",
    "## Leveraging Ollama Capabilities\n",
    "**Ollama** is a framework designed to simplify the deployment and use of LLaMA models, offering us the ability to run these models locally or in cloud environments for practical applications.\n",
    "\n",
    "## Using Ollama with LangChain for the app development\n",
    "Utilizing these two frameworks together allows us to create powerful applications that can leverage the capabilities of LLaMA models while also providing robust monitoring and debugging features through LangChain.\n",
    "In this **Jupyter Notebook**, we describe the development of a RAG application designed to answer questionsn about retrieved documents related to concert tours, thereby providing a consise and accurate response to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348672e2",
   "metadata": {},
   "source": [
    "First, we will install the required packages for the project, shown in the `requirements` section below:\n",
    "```bash\n",
    "    streamlit\n",
    "    langchain-ollama\n",
    "    langchain-chroma\n",
    "    transformers\n",
    "    langchain-core\n",
    "    langchain-text-splitters\n",
    "    langchain\n",
    "    torch\n",
    "    serpapi\n",
    "    google-search-results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eaac83",
   "metadata": {},
   "source": [
    "## 1. Importing necessary libraries for text processing and vector storage\n",
    "- `re`: Regular expressions for text manipulation.\n",
    "- `Document`: Class for handling documents in the LangChain framework.\n",
    "- `CharacterTextSplitter`: Class for splitting text into smaller chunks based on characters.\n",
    "- `Chroma`: Class for storing and managing vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbfea203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import traceback\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984a11e",
   "metadata": {},
   "source": [
    "## 2. Importing the library to create flexible chat prompts for LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed687bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f33e7e",
   "metadata": {},
   "source": [
    "## 3. Importing libraries to create a retrieval chain. Chains are used to combine multiple components into a single workflow.\n",
    "Chains encode sequencies of calls to LLMs, tools, and other chains.\n",
    "In this case, we will create a retrieval chain that **retrieves relevant documents** from a vector store and then generates a response based on those documents.\n",
    "The **retrieval chain** will use a retriever to find relevant documents and then use a language model to generate a response based on those documents.\n",
    "The **history aware retriever** will be used to keep track of the conversation history and use it to improve the retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5c0eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda733f",
   "metadata": {},
   "source": [
    "## 4. Importing Streamlit to create the web app UI for handling user input and SerpAPI for web search and scraping events from Google Events engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2594f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from serpapi import GoogleSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c4ee3",
   "metadata": {},
   "source": [
    "## 5. Importing LangChain Ollama\n",
    "This service uses the Ollama library to generate embeddings and chat completions using Ollama models. OllamaEmbeddings is used to create embeddings for the documents and to store them in a vector store. The ChatOllama class is used to generate chat completions using the Ollama model. The AIMessage class is used to create messages for the chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_core.messages import AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac77325",
   "metadata": {},
   "source": [
    "## 6. Setting the SerApi API key to use the engine and retrieve events\n",
    "SepApi uses the SerpApi API key to access the Google Search API. You can get your own API key from https://serpapi.com/manage-api-key. Make sure to set the environment variable SERPAPI_API_KEY to your own API key before running this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPAPI_API_KEY = \"\" # Replace with your own API key\n",
    "os.environ[\"SERPAPI_API_KEY\"] = SERPAPI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b2bee7",
   "metadata": {},
   "source": [
    "## 7. Configuring models for the embeddings and LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f15e1c",
   "metadata": {},
   "source": [
    "### 7.1. Embeddings\n",
    "The embeddings model is used to convert text into vector representations. This is important for semantic search and retrieval tasks.\n",
    "The `OllamaEmbeddings` class is used to create embeddings using the Ollama model. The `model` parameter specifies the name of the model to use. In this case, we are using the `nomic-embed-text` model, which is designed for text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e33f5809",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10af65",
   "metadata": {},
   "source": [
    "### 7.2. LLM\n",
    "The LLM (Language Model) is used to generate text based on the input provided. The `ChatOllama` class is used to create a chat-based language model using the Ollama model. The `model` parameter specifies the name of the model to use. In this case, we are using the `llama3.1:8b` model, which is a variant of the LLaMA model with 8 billion parameters. The `grounding` parameter specifies the grounding method to use. In this case, we are using \"strict\" grounding, which means that the model will be more focused on the input context.\n",
    "The LLM will also be used to generate concise summaries of the uploaded documents and scraped events. Summaries will then be saved in the Chroma vector store to be used for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "688476b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.1:8b\", grounding=\"strict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e7be6",
   "metadata": {},
   "source": [
    "**Important**: `nomic-embed-text` and `llama3.1:8b` need to be downloaded first by installing [Ollama](https://ollama.com/download) in your system and running the following commands:\n",
    "```bash\n",
    "ollama pull nomic-embed-text\n",
    "ollama pull llama3.1:8b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3f32c",
   "metadata": {},
   "source": [
    "## 8. Functions for Document Processing and Vector Base Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236cb64f",
   "metadata": {},
   "source": [
    "### 8.1. Function to check if the text is concert-related\n",
    "This function checks if the text contains any concert-related keywords. If the text is related to concerts, the app will not load the document, ensuring that only relevant information is processed.\n",
    "This is important for maintaining the focus of the application and ensuring that the user receives accurate and relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c169697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_concert_related(text, CONCERT_RELATED_KEYWORDS):\n",
    "    text_lower = text.lower() # Convert to lowercase for case-insensitive matching\n",
    "    return any(keyword in text_lower for keyword in CONCERT_RELATED_KEYWORDS)\n",
    "\n",
    "CONCERT_RELATED_KEYWORDS = [\"concert\", \"music\", \"band\", \"performance\", \"stage\", \n",
    "                            \"ticket\", \"venue\", \"artist\", \"festival\", \"tour\", \"gig\", \n",
    "                            \"show\", \"orchestra\", \"symphony\", \"recital\", \"live music\", \n",
    "                            \"audience\", \"encore\", \"setlist\", \"soundcheck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7f694",
   "metadata": {},
   "source": [
    "### 8.2. Function to create a Chroma vector store\n",
    "This function takes a text input and an embeddings model as arguments. It processes the text to remove unnecessary whitespace and non-alphanumeric characters, splits the text into smaller chunks, and creates a Chroma vector store using the provided embeddings model.\n",
    "We will use `RecursiveCharacterTextSplitter` to split the text into smaller chunks as the concert documents consist of multiple paragraphs. This way we will maintain each paragraph semantically intact and avoid splitting them into smaller pieces.\n",
    "\n",
    "`nomic-embed-text` model is used to convert text into vector representations and store them in a vector database. By generating vector representations of the text, the app can efficiently search and retrieve relevant information based on user queries.\n",
    "This is particularly useful for applications that require fast and accurate information retrieval, such as chatbots, search engines, and recommendation systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1519a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(text, embeddings):\n",
    "    processed_text = re.sub(r'\\s+', ' ', text)\n",
    "    processed_text = re.sub(r'[^a-zA-Z0-9\\s]', '', processed_text)\n",
    "    documents = [Document(page_content=processed_text)] # list of Document objects\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,\n",
    "                                                   chunk_overlap=20,\n",
    "                                                   length_function=len)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "    vector_store = Chroma.from_texts(texts=chunk_texts,\n",
    "                                     embedding=embeddings,\n",
    "                                     persist_directory=\"chroma_db\",     # directory to persist the database\n",
    "                                     collection_name=\"my_collection\")   # name of the collection\n",
    "    \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ba3f4",
   "metadata": {},
   "source": [
    "## 9. Function to generate summary\n",
    "This function utilizes an llm to generate a summary of the concert document based on the provided instructions. \n",
    "The instructions are designed to guide the llm in creating a concise and informative summary that captures the main points and key events of the concert document.\n",
    "For this purpose, we use the `ChatPromptTemplate` class to create a prompt template that includes the instructions and the text to be summarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c66f5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, instructions, llm):\n",
    "    summary_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Summarize the following concert document based ONLY on these instructions: {instructions}.\n",
    "        Document: {text}\n",
    "        \"\"\"\n",
    "    )\n",
    "    try:\n",
    "        chain = summary_prompt | llm\n",
    "        response = chain.invoke({\"instructions\": instructions, \"text\": text})\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating summary: {e}\")\n",
    "\n",
    "    return response.content.strip()\n",
    "\n",
    "SUMMARY_INSTRUCTIONS = \"\"\"\n",
    "    You are a concert summarizer. Your task is to summarize the concert document based on the given instructions.\n",
    "    You should focus on the main points, key events, and any important details that are relevant to the concert.\n",
    "    The summary should be concise and informative, providing a clear overview of the concert document.\n",
    "    You should not include any personal opinions or subjective interpretations.\n",
    "    Your summary should be in a clear and easy-to-understand format, using simple language and avoiding jargon.\n",
    "    If the document appears to be in JSON format, you should extract the relevant information and summarize it accordingly\n",
    "    and not mention the JSON format in the summary.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f41cb",
   "metadata": {},
   "source": [
    "## 10. Functions to create RAG (Retrieval-Augmented Generation) chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482bed8",
   "metadata": {},
   "source": [
    "### 10.1. Function to create a retrieval chain\n",
    "This function creates a retrieval chain using the provided vector store and language model.\n",
    "The retrieval chain is responsible for retrieving relevant documents from the vector store based on the user's query.\n",
    "\n",
    "- `ChatPromptTemplate` is used here to create a prompt template for the language model to generate a precise response based on the user's input and the retrieved documents.\n",
    "\n",
    "- `create_history_aware_retriever` is used to create a retriever that takes into account the conversation history, allowing the model to generate more contextually relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80742ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever_chain(vector_store, llm):\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})   # retrieve 5 most similar documents\n",
    "    prompt = ChatPromptTemplate.from_messages([MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "                                               (\"user\",\"{input}\"), \n",
    "                                               (\"user\",\"Given the following conversation, answer the question\")])\n",
    "    history_retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "    return history_retriever_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f9315",
   "metadata": {},
   "source": [
    "### 10.2. Function to get the conversational RAG chain\n",
    "This function creates a conversational RAG chain using the provided history retriever chain and language model.\n",
    "- `ChatPromptTemplate` is used to create a prompt template for the language model, instructing the system to answer questions based on the context provided. Then a message placeholder is added to include the chat history, and finally the user input is included.\n",
    "- The `create_stuff_documents_chain` function is used to create a document chain that combines the retrieved documents and the language model.\n",
    "- The `create_retrieval_chain` function is then used to create a retrieval chain that combines the history retriever chain and the document chain.\n",
    "\n",
    "\n",
    "This way, the conversational RAG chain can retrieve relevant documents from the vector store and generate answers based on the context of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8313cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversational_rag(history_retriever_chain, llm):\n",
    "    answer_prompt = ChatPromptTemplate.from_messages([(\"system\",\"Answer the question based on the context below: \\n\\n{context}\"), \n",
    "                                                      MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "                                                      (\"user\",\"{input}\")])\n",
    "    document_chain = create_stuff_documents_chain(llm, answer_prompt)\n",
    "    conversational_retrieval_chain = create_retrieval_chain(history_retriever_chain, document_chain)\n",
    "\n",
    "    return conversational_retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00946d7b",
   "metadata": {},
   "source": [
    "### 10.3. Function to return the response from the chatbot\n",
    "\n",
    "This function takes the user input and the language model as arguments, and returns the response from the chatbot.\n",
    "It calls the `get_retriever_chain` and `get_conversational_rag` functions to create a retrieval chain and a conversational RAG chain.\n",
    "Then, it invokes the chain with the chat history and user input to get the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c6d2726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(user_input, llm):\n",
    "    formatted_chat_history = [message.content if hasattr(message, 'content') else str(message)\n",
    "                              for message in st.session_state.chat_history] # list of chat history messages\n",
    "    \n",
    "    history_retriever_chain = get_retriever_chain(st.session_state.vector_store, llm)\n",
    "    conversation_rag_chain = get_conversational_rag(history_retriever_chain, llm)\n",
    "    response = conversation_rag_chain.invoke({\n",
    "          \"chat_history\":formatted_chat_history,\n",
    "          \"input\":user_input\n",
    "      })\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b02c92",
   "metadata": {},
   "source": [
    "## 11. SerpAPI function to extract events for an artist\n",
    "\n",
    "This function uses the SerpAPI to search for events related to a specific artist.\n",
    "It takes the artist's name and your SerpAPI key as input and returns a list of events for that artist according to the parameters set in the function.\n",
    "Search results are edited to remove unnecessary columns and then formatted into a string to feed into the LLM. Events are limited to 3 because the LLM is not able to process larger amounts of text reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9943c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events_for_artist(artist_name, api_key):\n",
    "    params = {\n",
    "        \"api_key\": {api_key},\n",
    "        \"engine\": \"google_events\",\n",
    "        \"q\": {artist_name},\n",
    "        \"hl\": \"en\",\n",
    "        \"gl\": \"us\",\n",
    "        \"htichips\": \"concerts\"\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    events = results.get(\"events_results\", [])\n",
    "    # drop 2 last columns from list\n",
    "    for event in events:\n",
    "        event.pop(\"thumbnail\", None)\n",
    "        event.pop(\"image\", None)\n",
    "        event.pop(\"event_location_map\", None)\n",
    "\n",
    "    events_str = \"\\n\\n\\nEVENT\\n\".join(\n",
    "        \"\\n\".join(f\"{key}: {value}\" for key, value in event.items()) for event in events[:3]\n",
    "        ) # limit to 3 events and organize string formatting\n",
    "    return events_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a37e09b",
   "metadata": {},
   "source": [
    "### Initialization of chat history and vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c991703",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "vector_store=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739f4c7",
   "metadata": {},
   "source": [
    "## 12. Streamlit App Code\n",
    "Below is the code for the Streamlit app. It includes the UI elements and the main logic for handling user input and displaying responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35efa8f",
   "metadata": {},
   "source": [
    "### 12.1. Sidebar for Uploading Concert Documents and Searching for Concerts based on Artist Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc66cb5e",
   "metadata": {},
   "source": [
    "#### 12.1.1. On the left side of the app, we create a sidebar for uploading concert documents. The sidebar includes a text area for pasting the concert document and a button to upload it. If the user clicks the button, we check if the document is related to concerts using the `is_concert_related` function. If it is, we generate a summary and store it in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"<style>h2 {text-align: center;}</style>\" # centering the header\n",
    "st.markdown(style, unsafe_allow_html=True)\n",
    "st.header(\"LangChain Concert chatbot\")\n",
    "st.write(\"<p style='text-align: center;'>This app is a concert chatbot that can answer questions about concerts.</p>\", unsafe_allow_html=True)\n",
    "\n",
    "st.sidebar.subheader(\"Upload Concert Document\")\n",
    "doc_text = st.sidebar.text_area(\"Paste your concert document here\", height=200, key=\"doc_input\")\n",
    "if st.sidebar.button(\"Upload Document\", key=\"doc_submit_button\"):\n",
    "    if not doc_text:\n",
    "        st.warning(\"Please enter some text.\") # if no text is entered, show warning\n",
    "    elif not is_concert_related(doc_text, CONCERT_RELATED_KEYWORDS): # document relation check\n",
    "        st.warning(\"Sorry, I cannot ingest documents with other themes.\") # if document is not concert-related, won't ingest and show warning\n",
    "    else:\n",
    "        with st.spinner(\"Processing document...\"):\n",
    "            try:\n",
    "                summary = generate_summary(doc_text, SUMMARY_INSTRUCTIONS, llm) # generate summary\n",
    "                if \"chat_history\" not in st.session_state: # `st.session_state` is used to persist data across reruns\n",
    "                    st.session_state.chat_history = [] # list to store chat history.\n",
    "                if \"vector_store\" not in st.session_state:\n",
    "                    st.session_state.vector_store = get_vector_store(summary, embeddings) # create vector store\n",
    "                    st.session_state.last_text = summary # store the last text, which is the summary\n",
    "                st.success(\"Your document has been successfully added to the database.\")\n",
    "                st.write(summary)\n",
    "            except Exception as e: # handle any errors that occur during processing\n",
    "                st.error(f\"Error: {e}\")\n",
    "                print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a282c",
   "metadata": {},
   "source": [
    "#### 12.1.2. The sidebar also includes a text input for searching for concerts based on the artist's name. If the user clicks the button, we use the `get_events_for_artist` function to fetch concert events from Google Search and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.sidebar.subheader(\"Search Artist Events\")\n",
    "artist_name = st.sidebar.text_input(\"Enter artist name (e.g., Lady Gaga)\", key=\"artist_input\")\n",
    "if st.sidebar.button(\"Search Events\", key=\"event_submit_button\"):\n",
    "    if not artist_name:\n",
    "        st.warning(\"Please enter an artist name.\")\n",
    "    else:\n",
    "        with st.spinner(f\"Searching concerts for {artist_name}...\"):\n",
    "            try:\n",
    "                events = get_events_for_artist(artist_name, SERPAPI_API_KEY)\n",
    "                summary = generate_summary(events, SUMMARY_INSTRUCTIONS, llm)\n",
    "                if \"chat_history\" not in st.session_state:\n",
    "                    st.session_state.chat_history = []\n",
    "                if \"vector_store\" not in st.session_state:\n",
    "                    st.session_state.vector_store = get_vector_store(summary, embeddings)\n",
    "                    st.session_state.last_text = summary\n",
    "                st.success(f\"The upcoming events of {artist_name} have been successfully added to the database.\")\n",
    "                st.write(summary)\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error: {e}\")\n",
    "                print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec05f230",
   "metadata": {},
   "source": [
    "### 12.2. Chatbot Interface\n",
    "The chatbot interface allows users to ask questions about the concert document and get answers based on the ingested data.\n",
    "It uses the LangChain library to create the conversational retrieval-augmented generation system for accurate and relevant responses.\n",
    "AIMessage is used to format the AI's responses, and the chat history is maintained in the session state.\n",
    "The user input is taken from a text input box, and the response is generated using the `get_response` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ac2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = st.text_input(\"Ask a quenstion:\")\n",
    "if st.button(\"Submit\", key=\"question_submit_button\"):\n",
    "    if not user_input:\n",
    "        st.warning(\"Please enter a question.\")\n",
    "    else:\n",
    "        with st.spinner(\"Generating response...\"):\n",
    "            try:\n",
    "                if user_input is None and user_input == \"\":\n",
    "                    st.warning(\"Please enter a question.\")\n",
    "                else:\n",
    "                    response = get_response(user_input, llm)\n",
    "                    st.session_state.chat_history.append(AIMessage(content=response))\n",
    "                for message in st.session_state.chat_history:\n",
    "                    if isinstance(message, AIMessage):\n",
    "                        with st.chat_message(\"AI\"):\n",
    "                            st.write(message.content)\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error: {e}\")\n",
    "                print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345aab5f",
   "metadata": {},
   "source": [
    "## Coclusion\n",
    "We successfully created the concert chatbot using LangChain and Streamlit.\n",
    "It will prove to be a valuable tool for users looking to get information about concerts and events related to their favorite artists.\n",
    "The chatpot can reliably answer questions based on the ingested concert documents and provide information about upcoming events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bef4d0",
   "metadata": {},
   "source": [
    "### The implemented code was inspired by the Medium articles, LangChain documentation and the GitHub repository shown in the references below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46c6cc",
   "metadata": {},
   "source": [
    "## **References**\n",
    "* https://medium.com/@aminajavaid30/building-a-rag-system-the-data-ingestion-pipeline-d04235fd17ea\n",
    "* https://medium.com/@laddhaakshatrai/how-to-perform-data-ingestion-with-langchain-day-12-100-f11288d7ae99\n",
    "* https://www.hostinger.com/tutorials/what-is-ollama#Key_features_of_Ollama\n",
    "* https://medium.com/@danushidk507/rag-with-llama-using-ollama-a-deep-dive-into-retrieval-augmented-generation-c58b9a1cfcd3\n",
    "* https://medium.com/@jiangan0808/retrieval-augmented-generation-rag-with-open-source-hugging-face-llms-using-langchain-bd618371be9d\n",
    "* https://python.langchain.com/docs/tutorials/rag/\n",
    "* https://python.langchain.com/v0.2/docs/tutorials/local_rag/\n",
    "* https://medium.com/@mrcoffeeai/conversational-chatbot-trained-on-own-data-streamlit-and-langchain-a45ea5a9dc0f\n",
    "* https://github.com/y-pred/Langchain/blob/main/Langchain%202.0/RAG_Conversational_Chatbot.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Data)",
   "language": "python",
   "name": "data-science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
